---
title: "Stat 694 - Job Skills Project"
author: "Clancey Barrington"
date: "12/11/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

### Read in Data
```{r}
data_analyst <- read.csv("DataAnalyst.csv")
data_scientist <- read.csv("DataScientist.csv")
data_engineer <- read.csv("DataEngineer.csv")
business_analyst <- read.csv("BusinessAnalyst.csv")
```

### Clean the Data
#### Fix Florida
```{r}
library(tidyverse)
# part of the business analyst data set is messed up
# Locations in Florida were messed
remove_rate <- c("Jacksonville, FL", "Jacksonville Beach, FL", "Orange Park, FL", "Ponte Vedra Beach, FL", "Mayport, FL", "Fleming Island, FL")
business_analyst2 <- business_analyst %>% 
  filter(Rating %in% remove_rate) 

# rename columns so everything matches
business_analyst2 <- business_analyst2 %>% 
  select(-c(Competitors, Easy.Apply)) %>% 
  rename(Job.Title = X, Salary.Estimate = index, Job.Description = Job.Title,
         Rating = Salary.Estimate, Company.Name = Job.Description,
         Location = Rating, Headquarters = Company.Name, Size = Location,
         Founded = Headquarters, Type.of.ownership = Size, Industry = Founded,
         Sector = Type.of.ownership, Revenue = Industry, Competitors = Sector,
         Easy.Apply = Revenue)

# add back to business
business_analyst3 <- business_analyst %>%
  filter(!Rating %in% remove_rate) %>% 
  select(-c(X, index)) 
business_analyst <- rbind(business_analyst3, business_analyst2)
```
#### Remove Uneeded Columns and Make Big Dataframe
```{r}
# remove unneeded columns
data_analyst <- data_analyst %>% 
  select(-c(X, Competitors))
data_scientist <- data_scientist %>% 
  select(-c(X, index, Competitors))
data_engineer <- data_engineer %>% 
  select(-c(Competitors))
business_analyst <- business_analyst %>% 
  select(-c(Competitors))

# add job posting category to dataframe
data_analyst['job_type'] = 'data analyst'
data_scientist['job_type'] = 'data scientist'
data_engineer['job_type'] = 'data engineer'
business_analyst['job_type'] = 'business analyst'

# combine data sets
jobs_data <- rbind(data_analyst, data_scientist, data_engineer, business_analyst)

# fix column names don't like periods
library(janitor)

jobs_data <- jobs_data %>%
  clean_names()
```

#### Take Care of NA's/ Fix Salary Est. and Company Name
```{r}
library(naniar)

jobs_data$rating <- as.double(jobs_data$rating)

# find missing data and replace with NA
jobs_data <- jobs_data %>%
  replace_with_na(replace = list(salary_estimate = "-1", 
                                 rating = "-1",
                                 company_name = c("", "1"),
                                 location = "Unknown",
                                 headquarters = "-1",
                                 size = c("-1", "Unknown"),
                                 founded = "-1",
                                 type_of_ownership = c("-1", "Unknown"),
                                 industry = c("-1", "Unknown / Non-Applicable"),
                                 sector = "-1",
                                 revenue = c("-1", "Unknown / Non-Applicable", "True")
                                 ))

# recode easy apply to False 0, and True 1
jobs_data$easy_apply <- ifelse(jobs_data$easy_apply == "True",1,0)

# fix up the values in the data
# salary estimate remove (Glassdoor est.) and Empy
# remove rating from company name
jobs_data <- jobs_data %>% 
  mutate(salary_estimate = as.character(gsub("[(Glassdoor est.)]", "", salary_estimate))) %>% 
  mutate(salary_estimate = as.character(gsub("Empy", "", salary_estimate))) %>% 
  mutate(company_name = as.character(gsub("[[:digit:]]+$", "", company_name))) %>% 
  mutate(company_name = as.character(gsub("[[:punct:]]+$", "", company_name))) %>% 
  mutate(company_name = as.character(gsub("[[:digit:]]+$", "", company_name))) %>% 
  mutate(company_name = as.character(gsub("\n", "", company_name)))
```

#### Get Salary Mid and State
```{r}
#get rid of hourly pay
# make salary numeric
# make salary range only have numbers
# get the min salary
# get the max salary
# get the midpoint salary
jobs_data <- jobs_data %>%
  mutate(desc_number = row_number()) %>% 
  filter(!grepl("PHu",salary_estimate)) %>% 
  mutate(salary = as.numeric(gsub("[^\\d]+", "", salary_estimate, perl=TRUE))) %>% 
  mutate(min = ifelse(nchar(salary) == 4, floor(salary / 100), 
                ifelse(nchar(salary) == 5, floor(salary / 1000), floor(salary / 1000)))) %>% 
  mutate(max = ifelse(nchar(salary) == 4, salary %% 100, 
                ifelse(nchar(salary) == 5, salary %% 1000, salary %% 1000))) %>% 
  mutate(mid = (min + max) / 2) 

# get the state
jobs_data$state <- str_sub(jobs_data$location,-2,-1)
jobs_data$state <- as.factor(jobs_data$state)

# remove united kingdom rows
jobs_data<- jobs_data %>%
  filter(!grepl("om",state))

```

### EDA
#### Look at Missing Data

Only 8% of my data is missing and there is no missing data in job description.
```{r}
# look at missing data
library(Amelia)
missmap(jobs_data, main = "Missing values vs observed")

# I created a report but commented it out
library(DataExplorer)
#create_report(jobs_data)
```
#### Company Rating
The average company rating is approximately 3.8. The distribution of scores is skewed left, but looks roughly normal.
```{r}
# avg. company rating
summary(jobs_data$rating)
# distribution of company rating
ggplot(jobs_data, aes(x = rating)) +
  geom_histogram(fill = "red") +
  labs(title = "Distribution of Company Rating", x = "Company Rating", y = "Count")
```
#### Salary Midpoint
The salary midpoints are skewed to the right.
```{r, message=FALSE,warning=FALSE}
# avg. company rating
summary(jobs_data$mid)

# distribution of mid salary
ggplot(jobs_data, aes(x = mid)) +
  geom_histogram(fill = "#35785f") +
  labs(title = "Distribution of Salary Midpoint", x = "Salary (x1,000)", y = "Count")


# salary boxplots by job type
ggplot(jobs_data, aes(x=job_type, y=mid)) + 
  geom_boxplot(fill = "#35785f")+
  labs(title = "Salary by Job Type", x = "Job Type", y = "Salary (x1,000)")


# mean salary by job type
jobs_data %>% 
  group_by(job_type) %>% 
  summarise(mean_salary = mean(mid, na.rm = TRUE), median_salary = median(mid, na.rm = TRUE), min_salary = min(mid, na.rm = TRUE), max_salary = max(mid, na.rm = TRUE))

# mean salary by job type and state
jobs_data %>% 
  group_by(state, job_type) %>% 
  summarise(mean_salary = mean(mid, na.rm = TRUE), median_salary = median(mid, na.rm = TRUE), min_salary = min(mid, na.rm = TRUE), max_salary = max(mid, na.rm = TRUE)) 

```
#### Most Featured Companies
The top 5 featured companies in this data set are Kforce, Amazon, Divers Lynx, Apple, and Staffigo Technical Services, LLC.
```{r}
# companies that occur the most
tail(names(sort(table(jobs_data$company_name))), 5)
```
#### Most Featured Locations
The top 5 featured locations in this data set are Houston, TX, San Diego, CA, Chicago, IL, Austin, TX, and New York, NY.
```{r}
# companies that occur the most
tail(names(sort(table(jobs_data$location))), 5)
```
#### Most Featured Sectors
The top 5 featured sectors in this data set are Biotech & Pharmaceuticals, Health Care, Finance, Business Services, and Information Technology.
```{r}
# companies that occur the most
tail(names(sort(table(jobs_data$sector))), 5)
```
#### Most Featured Company Types
The top 5 featured company types in this data set are Government, Nonprofit Organization, Subsidiary or Business Segment, Company - Public, and Company - Private.
```{r}
# companies that occur the most
tail(names(sort(table(jobs_data$type_of_ownership))), 5)
```
#### Company Size Bar Graphs
Of the companies that have size listed, most have 10000+ employees.
```{r}
jobs_data %>% 
  drop_na(size) %>%
  ggplot(aes(x = factor(size, level = c('1 to 50 employees', '51 to 200 employees', '201 to 500 employees', 
                                        '501 to 1000 employees', '1001 to 5000 employees', '5001 to 10000 employees',
                                        '10000+ employees')))) +
  geom_bar(fill = "orange") +
  labs(title = "Size of Companies", x = "Size", y = "Count") +
  coord_flip()
```
### Text Mining
To put my data in tidy text formula I followed [Text Mining with R: A Tidy Approach](https://www.tidytextmining.com/tidytext.html) Ch. 1 The tidy text format.
#### Preparing the Data
```{r}
library(tidytext)
# was coded as a factor needs to be a character
jobs_data$job_description <- as.character(jobs_data$job_description)

job_desc_words <- jobs_data %>% 
  select(job_description, job_type)

# â and Â instead of spaces and put a space for / because R/Python was listed in some
job_desc_words$job_description <- gsub("[â]+", " ", job_desc_words$job_description)
job_desc_words$job_description <- gsub("[Â]+", " ", job_desc_words$job_description)
job_desc_words$job_description <- gsub("[/]+", " ", job_desc_words$job_description)
job_desc_words$job_description <- gsub("[.]+", " ", job_desc_words$job_description)

# add description number so I have no duplicate words in a listing 
original_desc <- job_desc_words %>%
  mutate(desc_number = row_number()) %>%
  ungroup() 

tidy_desc <- original_desc %>%
  unnest_tokens(word, job_description) 

data(stop_words)

# stopwords had r and c in it so I removed it
stop_words <- stop_words[!stop_words$word == "r",]
stop_words <- stop_words[!stop_words$word == "c",]

# take out stop words
tidy_desc <- tidy_desc %>%
  anti_join(stop_words) 

# remove duplicate rows
tidy_desc <- tidy_desc[!duplicated(tidy_desc), ]
```

#### Most used words
```{r}
# bar graph of most used words
tidy_desc %>%
  count(word, sort = TRUE) %>%
  filter(n > 7450) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "#3a6a8a") +
  xlab(NULL) +
  coord_flip()+
  labs(title = "Top 10 Words in Job Descriptions", y = "Count")


# most used for data analyst
tidy_desc %>%
  filter(job_type == "data analyst") %>% 
  count(word, sort = TRUE) %>%
  filter(n > 1357) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "yellow") +
  xlab(NULL) +
  coord_flip()+
  labs(title = "Top 10 Words in Data Analyst Job Descriptions", y = "Count")

# most used for data scientist
tidy_desc %>%
  filter(job_type == "data scientist") %>% 
  count(word, sort = TRUE) %>%
  filter(n > 2260) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "yellow") +
  xlab(NULL) +
  coord_flip()+
  labs(title = "Top 10 Words in Data Scientist Job Descriptions", y = "Count")

# most used for data engineer
tidy_desc %>%
  filter(job_type == "data engineer") %>% 
  count(word, sort = TRUE) %>%
  filter(n > 1534) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "yellow") +
  xlab(NULL) +
  coord_flip() +
  labs(title = "Top 10 Words in Data Engineer Job Descriptions", y = "Count")

# most used for business analyst
tidy_desc %>%
  filter(job_type == "business analyst") %>% 
  count(word, sort = TRUE) %>%
  filter(n > 2569) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "yellow") +
  xlab(NULL) +
  coord_flip()+
  labs(title = "Top 10 Words in Business Analyst Job Descriptions", y = "Count")
```

#### Word Cloud
```{r}
library(wordcloud)
library(RColorBrewer)
tidy_desc2 <- tidy_desc
pal <- brewer.pal(9,"Paired")
tidy_desc2$word <- gsub("[0-9]+", "", tidy_desc2$word)
tidy_desc2 %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, colors = pal))

# data analyst word cloud
tidy_desc2 %>%
  filter(job_type == "data analyst") %>% 
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, colors = pal))

# data scientist word cloud
tidy_desc2 %>%
  filter(job_type == "data scientist") %>% 
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, colors = pal))

# data engineer word cloud
tidy_desc2 %>%
  filter(job_type == "data engineer") %>% 
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, colors = pal))

# business analyst word cloud
tidy_desc2 %>%
  filter(job_type == "business analyst") %>% 
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, colors = pal))
```
#### Postings that have remote
```{r}
# see how many have remote in them
# about 11% have remote in them
tidy_desc %>% 
  filter(str_detect(word, "remote")) %>% 
  count(word, sort = TRUE)
```
### Bigrams
To create the bigrams and trigrams I followed [Text Mining with R: A Tidy Approach](https://www.tidytextmining.com/ngrams.html) Ch.4 Relationships between words: n-grams and correlations.
```{r}
job_bigrams <- original_desc %>%
  unnest_tokens(bigram, job_description, token = "ngrams", n = 2)

#seperate bigrams to remove stop words
bigrams_separated <- job_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

# get words that have skills after them
bigram_counts %>% 
  filter(word2 == "skills")

# get words that have experience after them
bigram_counts %>% 
  filter(word2 == "experience")

# get words that have degree after them
bigram_counts %>% 
  filter(word2 == "degree")

# unite the words
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")
```
#### Bigram word cloud
```{r}
bigrams_united %>%
  count(bigram) %>%
  with(wordcloud(bigram, n, max.words = 50, colors = pal))
```
#### Bigram Graph
```{r}
library(igraph)

#create graph
bigram_graph <- bigram_counts %>%
  filter(n > 1500) %>%
  graph_from_data_frame()

# graph it
library(ggraph)
set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```
### Trigrams
```{r}
job_trigrams <- original_desc %>%
  unnest_tokens(trigram, job_description, token = "ngrams", n = 3)

# seperate to remove stop words
trigrams_separated <- job_trigrams %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

trigrams_filtered <- trigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word3 %in% stop_words$word)

# new trigram counts:
trigram_counts <- trigrams_filtered %>% 
  count(word1, word2, word3, sort = TRUE)

# get trigrams that end in skills
trigram_counts %>% 
  filter(word3 == "skills")

# get trigrams that end in experience
trigram_counts %>% 
  filter(word3 == "experience")

# put all back together
trigrams_united <- trigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ") 
```
#### Trigram wordcloud
```{r}
trigrams_united %>%
  count(trigram) %>%
  with(wordcloud(trigram, n, max.words = 50, colors = pal))
```
#### Trigram graph
```{r}
# make the graph
trigram_graph <- trigram_counts %>%
  filter(n > 300) %>%
  graph_from_data_frame()

# graph it
set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(trigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```
### Job Skills
#### Soft Skills
```{r}
# look for occurance of word in tidy_desc if one word
# look for occurance of word in the bigram if two words
comm <- tidy_desc %>% 
  filter(str_detect(word, 'communication')) %>% 
  rename(bigram = word)
comm['skill'] = 'communication'

proj_man <- bigrams_united %>%
  filter(str_detect(bigram, "project management"))
proj_man['skill'] = 'project management'

analytical <- tidy_desc %>%
  filter(str_detect(word, "analytical"))%>% 
  rename(bigram = word)
analytical['skill'] = 'analytical'

interpersonal <- tidy_desc %>%
  filter(str_detect(word, "interpersonal"))%>% 
  rename(bigram = word)
interpersonal['skill'] = 'interpersonal'

time_man <- bigrams_united %>%
  filter(str_detect(bigram, "time manage"))
time_man['skill'] = 'time management'

# look for different ways a phrase can be said
crit_think1 <- bigrams_united %>%
  filter(str_detect(bigram, "critical think"))
crit_think2 <- bigrams_united %>%
  filter(str_detect(bigram, "think critically"))
crit_think3 <- bigrams_united %>%
  filter(str_detect(bigram, "critically think"))
crit_think <- rbind(crit_think1, crit_think2, crit_think3)     
crit_think['skill'] = 'critical thinking'

organization1 <- bigrams_united %>%
  filter(str_detect(bigram, "organization skill"))
organization2 <- tidy_desc %>%
  filter(str_detect(word, "organized")) %>% 
  rename(bigram = word)
organization3 <- bigrams_united %>%
  filter(str_detect(bigram, "organizational skill"))
organization <- rbind(organization1, organization2, organization3)
organization['skill'] = 'organization'

creative <- tidy_desc %>%
  filter(str_detect(word, "creativ"))%>% 
  rename(bigram = word)
creative['skill'] = 'creativity'

leadership <- tidy_desc%>%
  filter(str_detect(word, "leadership"))%>% 
  rename(bigram = word)
leadership['skill'] = 'leadership'

# put them all together
soft_skills <- rbind(comm, proj_man, analytical, interpersonal, time_man, crit_think, organization, creative, leadership)

# get the desc number and skill only to remove duplicates
soft_skills <- soft_skills %>% 
  select(desc_number, skill)
soft_skills <- soft_skills[!duplicated(soft_skills), ]

# get percentage
percent_soft <- soft_skills %>% 
  group_by(skill) %>% 
  summarise(n = n())  %>% 
  mutate(proportion = n / nrow(jobs_data)) 

# plot bar graph
percent_soft %>% 
  ggplot(aes(x = skill, y = proportion)) +
  geom_bar(stat = "identity", fill = "#492a8c") +
  labs(title = "Most Desired Soft Skills") +
  coord_flip()

```
#### hard skills
```{r}
comp_sci1 <- bigrams_united %>%
  filter(str_detect(bigram, "computer science"))
comp_sci2 <- bigrams_united %>%
  filter(str_detect(bigram, "comp sci"))
comp_sci <- rbind(comp_sci1, comp_sci2)
comp_sci['skill'] = 'computer science'

mach_learn1 <- bigrams_united %>%
  filter(str_detect(bigram, "machine learn"))
mach_learn2 <- tidy_desc %>%
  filter(word == "ml") %>% 
  rename(bigram = word)
mach_learn <- rbind(mach_learn1, mach_learn2)
mach_learn['skill'] = 'machine learning'

business_int <- bigrams_united %>%
  filter(str_detect(bigram, "business intel"))
business_int['skill'] = 'business intelligence'

data_analysis <- bigrams_united %>%
  filter(str_detect(bigram, "data analysis"))
data_analysis['skill'] = 'data analysis'

statistics <- tidy_desc %>%
  filter(str_detect(word, "statistics")) %>% 
  rename(bigram = word)
statistics['skill'] = 'statistics'

data_vis <- bigrams_united %>%
  filter(str_detect(bigram, "data visual"))
data_vis['skill'] = 'data visualization'

programming <- tidy_desc %>%
  filter(str_detect(word, "programming")) %>% 
  rename(bigram = word)
programming['skill'] = 'programming'

soft_eng <- bigrams_united %>%
  filter(str_detect(bigram, "software engineer"))
soft_eng['skill'] = 'software engineering'

hard_skills <- rbind(comp_sci, mach_learn, business_int, data_analysis, statistics, data_vis, programming, soft_eng)
hard_skills <- hard_skills %>% 
  select(desc_number, skill)
hard_skills <- hard_skills[!duplicated(hard_skills), ]

percent_hard <- hard_skills %>% 
  group_by(skill) %>% 
  summarise(n = n())  %>% 
  mutate(proportion = n / nrow(jobs_data)) 

percent_hard %>% 
  ggplot(aes(x = skill, y = proportion)) +
  geom_bar(stat = "identity", fill = "#492a8c") +
  labs(title = "Most Desired Hard Skills") +
  coord_flip()

```
#### languages
```{r}
r <- tidy_desc %>%
  filter(word == "r") 
r['skill'] = 'r'

python <- tidy_desc %>%
  filter(str_detect(word, "python"))
python['skill'] = 'python'

c <- tidy_desc %>%
  filter(word == "c") 
c['skill'] = 'c++/ c'

java <- tidy_desc %>%
  filter(word == "java")
java['skill'] = 'java'

sas <- tidy_desc %>%
  filter(word == "sas")
sas['skill'] = 'sas'

scala <- tidy_desc %>%
  filter( word == "scala")
scala['skill'] = 'scala'

sql <- tidy_desc %>%
  filter(str_detect(word, "sql"))
sql['skill'] = 'sql'

lang_skills <- rbind(r, python, c, java, sas, scala, sql)
lang_skills <- lang_skills %>% 
  select(desc_number, skill)
lang_skills <- lang_skills[!duplicated(lang_skills), ]

percent_lang <- lang_skills %>% 
  group_by(skill) %>% 
  summarise(n = n())  %>% 
  mutate(proportion = n / nrow(jobs_data)) 

percent_lang %>% 
  ggplot(aes(x = skill, y = proportion)) +
  geom_bar(stat = "identity", fill = "#35785f") +
  labs(title = "Most Desired Language") +
  coord_flip()

```

#### tools
```{r}
tableau <- tidy_desc %>%
  filter(str_detect(word, "tableau"))
tableau['skill'] = 'tableau'

excel <- tidy_desc %>%
  filter(word == "excel")
excel['skill'] = 'excel'

microsoft <- tidy_desc %>%
  filter(str_detect(word, "microsoft")) 
microsoft['skill'] = 'microsoft'

hadoop <- tidy_desc %>%
  filter(str_detect(word, "hadoop"))
hadoop['skill'] = 'hadoop'

spark <- tidy_desc %>%
  filter(str_detect(word, "spark"))
spark['skill'] = 'spark'

bi <- tidy_desc %>%
  filter(word == "bi") 
bi['skill'] = 'power bi'

aws1 <- tidy_desc %>%
  filter(word == "aws")
aws2 <- trigrams_united %>% 
  filter(str_detect(trigram, "amazon web service")) %>% 
  rename(word = trigram)
aws <- rbind(aws1, aws2)
aws['skill'] = 'aws'

tool_skills <- rbind(tableau, excel, microsoft, hadoop, spark, bi, aws)
tool_skills <- tool_skills %>% 
  select(desc_number, skill)
tool_skills <- tool_skills[!duplicated(tool_skills), ]

percent_tool <- tool_skills %>% 
  group_by(skill) %>% 
  summarise(n = n())  %>% 
  mutate(proportion = n / nrow(jobs_data)) 

percent_tool %>% 
  ggplot(aes(x = skill, y = proportion)) +
  geom_bar(stat = "identity", fill = "#35785f") +
  labs(title = "Most Desired Tools") +
  coord_flip()

```
#### degree
```{r}
assoc1 <- bigrams_united %>%
  filter(str_detect(bigram, "associates degree"))
assoc2 <- bigrams_united %>%
  filter(str_detect(bigram, "associate's degree"))
assoc3 <- bigrams_united %>%
  filter(str_detect(bigram, "associate degree"))
assoc4 <- bigrams_united %>%
  filter(str_detect(bigram, "college degree"))
assoc5 <- tidy_desc %>%
  filter(word == "aa") %>% 
  rename(bigram = word)
assoc <- rbind(assoc1, assoc2, assoc3, assoc4, assoc5)
assoc_box <- assoc %>% 
  select(desc_number) %>% 
  distinct()
assoc_box <- assoc_box[,1]
assoc["education"] = "associate's"

bach1 <- bigrams_united %>%
  filter(str_detect(bigram, "bachelor"))
bach2 <- bigrams_united %>%
  filter(str_detect(bigram, "undergraduate degree"))
bach_words <- c("bs", "ba")
bach3 <- tidy_desc %>%
  filter(word %in% bach_words) %>% 
  rename(bigram = word)
bach <- rbind(bach1, bach2, bach3)
bach_box <- bach %>% 
  select(desc_number) %>% 
  distinct()
bach_box <- bach_box[,1]
bach["education"] = "bachelor's"

mast1 <- bigrams_united %>%
  filter(str_detect(bigram, "masters degree"))
mast2 <- bigrams_united %>%
  filter(str_detect(bigram, "master's degree"))
mast3 <- bigrams_united %>%
  filter(str_detect(bigram, "master degree"))
mast4 <- bigrams_united %>%
  filter(str_detect(bigram, "advanced degree"))
mast5 <- bigrams_united %>%
  filter(str_detect(bigram, "post baccalaureate"))
mast_words <- c("ms", "ma", "mba")
mast6 <- tidy_desc %>%
  filter(word %in% mast_words)%>% 
  rename(bigram = word)
mast <- rbind(mast1, mast2, mast3, mast4, mast5, mast6)
mast_box <- mast %>% 
  select(desc_number) %>% 
  distinct()
mast_box <- mast_box[,1]
mast["education"] = "master's"

phd <- tidy_desc %>% 
  filter(word == "phd")%>% 
  rename(bigram = word)
doctorate <- tidy_desc %>% 
  filter(word == "doctorate")%>% 
  rename(bigram = word)
doctoral <- bigrams_united %>%
  filter(str_detect(bigram, "doctoral degree"))
phd_doc <- rbind(phd, doctorate, doctoral)
phd_doc_box <- phd_doc %>% 
  select(desc_number) %>% 
  distinct()
phd_doc_box <- phd_doc_box[,1]
phd_doc["education"] = "phd/ doctorate"

degrees <- rbind(assoc, bach, mast, phd_doc)
degrees <- degrees %>% 
  select(desc_number, education)
degrees <- degrees[!duplicated(degrees), ]

percent_degrees <- degrees %>% 
  group_by(education) %>% 
  summarise(n = n())  %>% 
  mutate(proportion = n / nrow(jobs_data)) 

percent_degrees %>% 
  ggplot(aes(x = education, y = proportion)) +
  geom_bar(stat = "identity", fill = "#3a6a8a") +
  labs(title = "Most Desired Degrees") +
  coord_flip()

```
#### degree boxplots
```{r}

edu_box <- jobs_data %>% 
  mutate(education = ifelse(desc_number %in% assoc_box, "associate's",
    ifelse(desc_number %in% bach_box, "bachelor's", 
    ifelse(desc_number %in% mast_box, "master's", 
    ifelse(desc_number %in% phd_doc_box, "phd/ doctorate", "not listed")))))

edu_box$education <- factor(edu_box$education,
    level = c("associate's", "bachelor's", "master's", "phd/ doctorate", "not listed", ordered = TRUE))

edu_box %>% 
  ggplot(aes(x = education, y = mid)) +
  geom_boxplot(fill = "#3a6a8a") +
  labs(title = "Pay by Education", x = "Education", y = "Salary (x1,000)")


edu_box %>% 
  group_by(education) %>% 
  summarise(mean_salary = mean(mid, na.rm = TRUE))
```
### Senior & entry level
```{r}
# tokenize the job title to see if a position is senior or entry level
title_words <- jobs_data %>% 
  select(job_title, job_type)

# â and Â instead of spaces and put a space for / because R/Python was listed in some
title_words$job_title <- gsub("[â]+", " ", title_words$job_title)
title_words$job_title <- gsub("[Â]+", " ", title_words$job_title)
title_words$job_title <- gsub("[/]+", " ", title_words$job_title)
title_words$job_title <- gsub("[.]+", " ", title_words$job_title)

# add description number so I have no duplicate words in a listing 
title_desc <- title_words %>%
  mutate(desc_number = row_number()) %>%
  ungroup() 

title_tidy_desc <- title_desc %>%
  unnest_tokens(word, job_title) 

data(stop_words)

# stopwords had r and c in it so I removed it
stop_words <- stop_words[!stop_words$word == "r",]
stop_words <- stop_words[!stop_words$word == "c",]

# take out stop words
title_tidy_desc <- title_tidy_desc %>%
  anti_join(stop_words) 

# remove duplicate rows
title_tidy_desc <- title_tidy_desc[!duplicated(title_tidy_desc), ]

entry1 <- bigrams_united %>%
  filter(str_detect(bigram, "entry level")) %>% 
  rename(word = bigram)
entry2 <- title_tidy_desc %>%
  filter(word == "entry") 
entry <- rbind(entry1, entry2)
entry_box <- entry %>% 
  select(desc_number) %>% 
  distinct()
entry_box <- entry_box[,1]
entry['level'] = 'entry level'

senior <- title_tidy_desc %>%
  filter(word == "senior") 
senior_box <- senior %>% 
  select(desc_number) %>% 
  distinct()
senior_box <- senior_box[,1]
senior['level'] = 'senior'

level_box <- jobs_data %>% 
  mutate(level = ifelse(desc_number %in% entry_box, "entry level",
    ifelse(desc_number %in% senior_box, "senior", 
    "junior/ not listed")))

level_box$level <- factor(level_box$level,
    level = c("entry level", "junior/ not listed", "senior", ordered = TRUE))

level_box %>% 
  ggplot(aes(x = level, y = mid)) +
  geom_boxplot() +
  labs(title = "Pay by Job Level", x = "Level", y = "Salary")

level_box %>% 
  group_by(level) %>% 
  summarise(mean_salary = mean(mid, na.rm = TRUE))
```


### LDA
To create my LDA model I followed [Text Mining with R: A Tidy Approach](https://www.tidytextmining.com/topicmodeling.html) Ch. 6 Topic modeling.
#### LDA Prep
```{r}
by_job <- jobs_data %>%
  select(job_type, job_description) 

#split into words
by_job_words <- by_job %>%
  unnest_tokens(word, job_description)

# document-word counts
word_counts <- by_job_words %>%
  anti_join(stop_words) %>%
  count(job_type, word, sort = TRUE) %>%
  ungroup()

# remove common words
common_words <- c("data", "experience", "skills")
word_counts <- word_counts %>% 
  filter(!word %in% common_words)
```

#### DTM
```{r}
# put into DocumentTermMatrix to do LDA
jobs_dtm <- word_counts %>%
  cast_dtm(job_type, word, n)

jobs_dtm
```

#### make model/ get top terms
```{r}
library(topicmodels)
jobs_lda <- LDA(jobs_dtm, k = 4, control = list(seed = 1234))
jobs_lda

# per-topic-per-word probabilities
job_topics <- tidy(jobs_lda, matrix = "beta")

top_terms <- job_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```
#### graph top terms
```{r}
library(ggplot2)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```

#### gamma boxplots
```{r}
jobs_gamma <- tidy(jobs_lda, matrix = "gamma")

jobs_gamma %>%
  mutate(title = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~document)
```
#### Top gamma values
```{r}
#look at the top 5 gamma values
job_classifications <- jobs_gamma %>%
  top_n(5, gamma) %>%
  ungroup()

job_classifications
```
#### look closer at data scientist and data analyst
```{r}
job_topics <- job_classifications %>%
  count(document, topic) %>%
  group_by(document) %>%
  top_n(1, n) %>%
  ungroup() %>%
  transmute(consensus = document, topic)

# see where consensus doesn't match
job_classifications %>%
  inner_join(job_topics, by = "topic") %>%
  filter(document != consensus)
```
#### assignment based on terms
```{r}
assignments <- augment(jobs_lda, data = jobs_dtm)
assignments

assignments <- assignments %>%
  inner_join(job_topics, by = c(".topic" = "topic"))

assignments %>% 
  head(10)
```
#### heatmap of predictions
```{r}
library(scales)

assignments %>%
  count(document, consensus, wt = count) %>%
  group_by(document) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(consensus, document, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "red", label = percent_format()) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1),
    panel.grid = element_blank()
  ) +
  labs(
    x = "Job words were assigned to",
    y = "Job words came from",
    fill = "% of assignments"
  )

# look at percentage of assignments
assignments %>%
  count(document, consensus, wt = count) %>%
  group_by(document) %>%
  mutate(percent = n / sum(n))
```

#### misassigned words
```{r}
wrong_words <- assignments %>%
  filter(document != consensus) 

wrong_words %>%
  count(document, consensus, term, wt = count) %>%
  ungroup() %>%
  arrange(desc(n)) %>% 
  head()
```

### Salary Classification
To choose my cutoff I used [Data Scientists and Mathematical Science Occupational Employment and Wages, May 2019](https://www.bls.gov/oes/current/oes152098.htm). I chose the median annual wage and rounded it up to $95,000.
#### Baseline
```{r}
# note that salary is mid times 1,000
jobs_ml <- jobs_data %>% 
  select(mid, job_description) %>% 
  drop_na() %>% 
  mutate(salary = ifelse(mid <= 95,1,0))

jobs_ml  <- jobs_ml  %>%
  mutate(desc_number = row_number()) %>%
  ungroup()

# get baseline
prop.table(table(jobs_ml$salary))
```
#### testing and training data
```{r}
library(text2vec)
library(data.table)
setDT(jobs_ml)
setkey(jobs_ml, desc_number)
set.seed(2016L)
all_ids = jobs_ml$desc_number
train_ids = sample(all_ids, 10205)
test_ids = setdiff(all_ids, train_ids)
train = jobs_ml[J(train_ids)]
test = jobs_ml[J(test_ids)]
```
#### logistic model w/ just vectorization
For all of the logistic regression models I followed [text2vec: Vectorization](http://text2vec.org/vectorization.html)
##### tokenize and lowercase training data
```{r}
# make training words lowercase and tokenize them
prep_fun <- tolower
tok_fun = word_tokenizer

it_train = itoken(train$job_description, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = jobs_ml$desc_number, 
             progressbar = FALSE)
vocab = create_vocabulary(it_train)

train_tokens = tok_fun(prep_fun(train$job_description))
it_train = itoken(train_tokens, 
                  ids = train$desc_number,
                  # turn off progressbar because it won't look nice in rmd
                  progressbar = FALSE)

vocab = create_vocabulary(it_train)
```
##### vectorize
```{r}
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
```
##### dimensions
```{r}
dim(dtm_train)
```
##### model
```{r}
library(glmnet)
NFOLDS = 4
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['salary']], 
                              family = 'binomial', 
                              # L1 penalty
                              alpha = 1,
                              # interested in the area under ROC curve
                              type.measure = "auc",
                              # 5-fold cross-validation
                              nfolds = NFOLDS,
                              # high value is less accurate, but has faster training
                              thresh = 1e-3,
                              # again lower number of iterations for faster training
                              maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))
```
##### auc
```{r}
plot(glmnet_classifier)
```
##### training auc
```{r}
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))

```
##### testing auc
```{r}
it_test = tok_fun(prep_fun(test$job_description))
it_test = itoken(it_test, ids = test$desc_number, 
                 # turn off progressbar because it won't look nice in rmd
                 progressbar = FALSE)

dtm_test = create_dtm(it_test, vectorizer)

preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$salary, preds)
```
#### logistic model with pruning and n grams
##### remove stop words
```{r}
t1 = Sys.time()
vocab = create_vocabulary(it_train, stopwords = stop_words$word)
print(difftime(Sys.time(), t1, units = 'sec'))
```
##### remove infrequent terms
```{r}
pruned_vocab = prune_vocabulary(vocab, 
                                 term_count_min = 10, 
                                 doc_proportion_max = 0.5,
                                 doc_proportion_min = 0.001)
vectorizer = vocab_vectorizer(pruned_vocab)
# create dtm_train with new pruned vocabulary vectorizer
t1 = Sys.time()
dtm_train  = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
```
##### dimensions
```{r}
dim(dtm_train)
```
##### create dtm for test
```{r}
dtm_test   = create_dtm(it_test, vectorizer)
dim(dtm_test)
```

##### use n grams
```{r}
t1 = Sys.time()
vocab = create_vocabulary(it_train, ngram = c(1L, 2L))
print(difftime(Sys.time(), t1, units = 'sec'))
```

##### make the model
```{r}
vocab = prune_vocabulary(vocab, term_count_min = 10, 
                         doc_proportion_max = 0.5)

bigram_vectorizer = vocab_vectorizer(vocab)

dtm_train = create_dtm(it_train, bigram_vectorizer)

t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['salary']], 
                 family = 'binomial', 
                 alpha = 1,
                 type.measure = "auc",
                 nfolds = NFOLDS,
                 thresh = 1e-3,
                 maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))

```
##### plot auc
```{r}
plot(glmnet_classifier)
```
##### max auc for training
```{r}
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
```
##### auc for testing data
```{r}
# apply vectorizer
dtm_test = create_dtm(it_test, bigram_vectorizer)
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$salary, preds)
```
#### logistic with hashing and n grams
##### hash training data
```{r}
h_vectorizer = hash_vectorizer(hash_size = 2 ^ 14, ngram = c(1L, 2L))

t1 = Sys.time()
dtm_train = create_dtm(it_train, h_vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
```
##### make model
```{r}
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['salary']], 
                             family = 'binomial', 
                             alpha = 1,
                             type.measure = "auc",
                             nfolds = 5,
                             thresh = 1e-3,
                             maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))
```
##### plot auc
```{r}
plot(glmnet_classifier)
```
##### max auc for training
```{r}
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
```
##### auc for testing
```{r}
dtm_test = create_dtm(it_test, h_vectorizer)

preds = predict(glmnet_classifier, dtm_test , type = 'response')[, 1]
glmnet:::auc(test$salary, preds)
```
#### logistic normalize and tf-idf
##### normalize training data
```{r}
dtm_train_l1_norm = normalize(dtm_train, "l1")
```
##### define tf-idf model
```{r}
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, vectorizer)

# define tfidf model
tfidf = TfIdf$new()
# fit model to train data and transform train data with fitted model
dtm_train_tfidf = fit_transform(dtm_train, tfidf)
# tfidf modified by fit_transform() call!
# apply pre-trained tf-idf transformation to test data
dtm_test_tfidf = create_dtm(it_test, vectorizer)
dtm_test_tfidf = transform(dtm_test_tfidf, tfidf)
```
##### fit model
```{r}
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train_tfidf, y = train[['salary']], 
                              family = 'binomial', 
                              alpha = 1,
                              type.measure = "auc",
                              nfolds = NFOLDS,
                              thresh = 1e-3,
                              maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))
```
##### plot auc
```{r}
plot(glmnet_classifier)
```
##### best auc for training
```{r}
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
```
##### auc for testing
```{r}
preds = predict(glmnet_classifier, dtm_test_tfidf, type = 'response')[,1]
glmnet:::auc(test$salary, preds)
```
#### naive bayes
For the naive bayes model I folllowed [Using Naïve Bayes algorithm for predictive classification](https://rpubs.com/dbrown/nbclass).
##### make corpus
```{r}
library(tm)
# make salary a factor
jobs_ml$salary <- factor(jobs_ml$salary)

# rearrange data
set.seed(2020)
jobs_ml <- jobs_ml[sample(nrow(jobs_ml)),]
job_corpus <- VCorpus(VectorSource(jobs_ml$job_description))
typeof(job_corpus) # Just to show that it is a list

```
##### print corpus
```{r}
print(job_corpus)
```
##### clean corpus
```{r}
job_corpus_clean <- job_corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords()) %>%
  tm_map(removePunctuation) %>%
  tm_map(stemDocument) %>%
  tm_map(stripWhitespace)
```
##### make dtm
```{r}
job_dtm <- DocumentTermMatrix(job_corpus_clean)
job_dtm
```

##### training and testing
```{r}
job_dtm_train <- job_dtm[1:10205, ]
job_dtm_test <- job_dtm[10205:12756, ]
job_train_labels <- jobs_ml[1:10205, ]$salary
job_test_labels <- jobs_ml[10205:12756, ]$salary
```

##### table training labels
```{r}
job_train_labels %>%
  table %>%
  prop.table
```

##### table testing labels
```{r}
job_test_labels %>%
  table %>%
  prop.table
```
##### use frequent terms
```{r}
job_dtm_freq_train <- job_dtm_train %>%
  findFreqTerms(5) %>%
  job_dtm_train[ , .]
job_dtm_freq_test <- job_dtm_test %>%
  findFreqTerms(5) %>%
  job_dtm_test[ , .]
```
##### convert counts
```{r}
convert_counts <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}

job_train <- job_dtm_freq_train %>%
  apply(MARGIN = 2, convert_counts)
job_test <- job_dtm_freq_test %>%
  apply(MARGIN = 2, convert_counts)

```
##### fit model
```{r}
library(e1071)
job_classifier <- naiveBayes(job_train, job_train_labels)
job_pred <- predict(job_classifier, job_test)
```
##### cross table
```{r}
library(gmodels)
CrossTable(job_pred, job_test_labels, prop.chisq = FALSE, chisq = FALSE, 
           prop.t = FALSE,
           dnn = c("Predicted", "Actual"))
```
##### trying to improve model
```{r}
job_classifier2 <- naiveBayes(job_train, job_train_labels, laplace = 1)
job_pred2 <- predict(job_classifier2, job_test)
CrossTable(job_pred2, job_test_labels, prop.chisq = FALSE, chisq = FALSE, 
           prop.t = FALSE,
           dnn = c("Predicted", "Actual"))
```
#### xgboost
For the xgboost model I followed the blog post [Explaining Black-Box Machine Learning Models - Code Part 2: Text classification with LIME](https://www.shirin-glander.de/2018/07/explaining_ml_models_code_text_lime/).
##### training and testing
```{r}
library(caret)
set.seed(42)
idx <- createDataPartition(jobs_ml$salary, 
                           p = 0.8, 
                           list = FALSE, 
                           times = 1)

job_train <- jobs_ml[ idx,]
job_test  <- jobs_ml[-idx,]
```
##### create dtm
```{r}
get_matrix <- function(text) {
  it <- itoken(text, progressbar = FALSE)
  create_dtm(it, vectorizer = hash_vectorizer())
}
dtm_train <- get_matrix(job_train$job_description)
dtm_test <- get_matrix(job_test$job_description)
```
##### fit model
```{r}
library(xgboost)
xgb_model <- xgb.train(list(max_depth = 7, 
                            eta = 0.1, 
                            objective = "binary:logistic",
                            eval_metric = "error", nthread = 1),
                       xgb.DMatrix(dtm_train, 
                                   label = job_train$salary == "1"),
                       nrounds = 50)
```
##### prediction and accuracy
```{r}
pred <- predict(xgb_model, dtm_test)

confusionMatrix(job_test$salary,
                as.factor(round(pred, digits = 0)))

```




